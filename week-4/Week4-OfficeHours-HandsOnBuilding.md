# Week 4 Office Hours: Hands-On Building, Testing & Peer Feedback
## Duration: 60 minutes | Format: Hands-on building workshop with peer collaboration

### Learning Objectives (from LearningOutcomes.md)
**Cognitive:**
- **Apply:** Build functional components using chosen AI tools
- **Analyze:** Troubleshoot implementation issues and compare approaches
- **Evaluate:** Assess whether implementations meet requirements

**Practical:**
- **Create:** Develop working features that meet defined requirements
- **Test:** Validate implementations against success criteria
- **Collaborate:** Provide constructive feedback on peer projects

### Target Audience
- **Beginners (30%):** Need hands-on guidance with implementation
- **Intermediate (50%):** Ready to build but need testing and feedback support
- **Advanced (20%):** Can help others and explore advanced implementation

---

## Session Structure

### Opening & Check-in (5 minutes)
**Facilitator Tasks:**
- Welcome everyone and check attendance
- Quick poll: Who has started building their capstone project? What challenges are you facing?
- Explain session structure: Build → Test → Get Feedback → Iterate
- Set expectations: This is hands-on, collaborative, and practical

**Participant Tasks:**
- Share one thing you've built so far (even if it's small)
- Mention your biggest implementation challenge
- Share what you hope to accomplish today

---

## Part 1: Building Sprint Setup (10 minutes)

### Individual Goal Setting (5 minutes)
**Participant Tasks:**
- Set specific building goal for this session
- Identify what you'll build/test/improve
- Choose appropriate AI tools for your task
- Prepare any materials or context you'll need

**Goal Setting Framework:**
```
Today I will:
- Build: [Specific component/feature]
- Test: [How I'll validate it works]
- Get feedback on: [Specific aspect]
- Improve: [What I'll iterate on]

Tools I'll use: [List your chosen AI tools]
Success criteria: [How I'll know I'm done]
```

### Implementation Planning (5 minutes)
**Facilitator Guidance:**
- Help participants choose realistic goals for 45 minutes
- Suggest breaking complex features into smaller components
- Encourage testing and iteration over perfection
- Provide guidance on tool selection for specific tasks

**Planning Questions:**
- What's the smallest functional component you can build?
- How will you test that it works?
- What feedback would be most helpful?
- How will you iterate based on results?

---

## Part 2: Hands-On Building Sprint (25 minutes)

### Individual Building Time (20 minutes)
**Participant Tasks:**
- Work on your chosen implementation goal
- Use advanced prompting techniques from this week
- Test outputs as you build
- Document what works and what doesn't

**Building Process:**
1. **Start building** your chosen component
2. **Test frequently** - don't wait until the end
3. **Iterate quickly** - fix issues as you find them
4. **Document progress** - note what techniques work
5. **Prepare for feedback** - get ready to share what you built

### Facilitator Support During Building
**Circulation and Assistance:**
- Move between participants to provide individual help
- Help with advanced prompting techniques
- Assist with testing and validation
- Troubleshoot implementation issues
- Encourage and provide guidance

**Common Support Areas:**
- Advanced prompting technique application
- Testing and quality assessment
- Tool-specific guidance
- Troubleshooting common issues
- Goal adjustment if needed

### Quick Check-ins (5 minutes)
**Mid-session Check-in:**
- How is everyone doing? Any major blockers?
- Quick wins to share?
- Need help with anything specific?

---

## Part 3: Testing and Validation (10 minutes)

### Individual Testing (5 minutes)
**Participant Tasks:**
- Test your implementation against success criteria
- Try different scenarios or inputs
- Identify what works well and what needs improvement
- Prepare to share results with peers

**Testing Framework:**
```
What I built: [Brief description]
How I tested it: [Testing approach]
What works well: [Successes]
What needs improvement: [Issues or gaps]
What I learned: [Key insights]
```

### Testing Strategies by Implementation Type

#### For Product Managers:
- **Requirements validation:** Does it meet user story criteria?
- **User flow testing:** Does the process work end-to-end?
- **Stakeholder perspective:** How would users/business react?

#### For Designers:
- **Usability testing:** Is it intuitive and accessible?
- **Visual consistency:** Does it match design standards?
- **User experience:** Does it solve the intended problem?

#### For Project Managers:
- **Process validation:** Does the workflow make sense?
- **Resource assessment:** Is it realistic to implement?
- **Risk evaluation:** What could go wrong?

### Group Testing Discussion (5 minutes)
**Quick Share-Out:**
- 2-3 volunteers share what they built and how they tested it
- Highlight testing approaches that worked well
- Share challenges and how they addressed them

---

## Part 4: Peer Feedback and Collaboration (15 minutes)

### Peer Review Setup (2 minutes)
**Facilitator Instructions:**
- Form groups of 3-4 people
- Each person shares their implementation
- Use structured feedback framework
- Focus on constructive suggestions and questions

### Structured Peer Review (10 minutes)
**Group Work Process:**
- Each person presents their implementation (2-3 minutes)
- Group provides structured feedback (2-3 minutes per person)
- Focus on improvement suggestions and questions
- Share techniques and approaches that worked

### Peer Review Framework

#### For Each Implementation, Evaluate:

**Functionality:**
- [ ] Does it work as intended?
- [ ] Are there any obvious bugs or issues?
- [ ] Does it meet the stated requirements?

**Implementation Quality:**
- [ ] Is the approach appropriate for the problem?
- [ ] Are AI tools being used effectively?
- [ ] Is the code/design clean and understandable?

**User Experience:**
- [ ] Would users find this useful?
- [ ] Is it easy to understand and use?
- [ ] Are there any usability concerns?

**Ethical Considerations:**
- [ ] Does it preserve user agency appropriately?
- [ ] Are ethical considerations addressed?
- [ ] Is AI involvement transparent?

#### Feedback Questions:
**Strengths:**
- What works really well about this implementation?
- What techniques or approaches are effective?
- What should be preserved or expanded?

**Improvements:**
- What could be enhanced or refined?
- Are there alternative approaches to consider?
- What testing or validation might help?

**Questions:**
- How did you handle [specific challenge]?
- What prompted you to use [specific technique]?
- Have you considered [alternative approach]?

### Group Reflection (3 minutes)
**Discussion Questions:**
- What implementation techniques worked best?
- What testing approaches were most effective?
- How did peer feedback help improve your approach?
- What will you try differently next time?

---

## Part 5: Iteration Planning (5 minutes)

### Individual Iteration Planning (3 minutes)
**Participant Tasks:**
- Based on testing and feedback, plan next steps
- Identify what to improve or try differently
- Set goals for continued work outside office hours
- Document lessons learned and techniques to try

**Iteration Planning Template:**
```
Based on today's work and feedback:

Immediate improvements (this week):
- [ ] [Specific improvement]
- [ ] [Specific improvement]

Next steps (next week):
- [ ] [Specific next step]
- [ ] [Specific next step]

Techniques to try:
- [ ] [Advanced prompting technique]
- [ ] [Testing approach]
- [ ] [Tool or method]

Lessons learned:
- [Key insight 1]
- [Key insight 2]
```

### Group Sharing and Support (2 minutes)
**Quick Share-Out:**
- One thing you'll improve based on today's feedback
- One technique you want to try next
- One question or challenge you still have

---

## Materials & Resources

### Implementation Planning Template
**File:** `Week4-ImplementationPlanning-Template.md`

**Sections:**
1. Goal setting framework
2. Implementation planning checklist
3. Testing and validation strategies
4. Iteration planning template

### Peer Review Framework
**File:** `Week4-PeerReview-Framework.md`

**Content:**
- Structured feedback questions
- Quality assessment criteria
- Best practices for giving and receiving feedback
- Common implementation patterns

### Troubleshooting Guide
**File:** `Week4-Troubleshooting-Guide.md`

**Content:**
- Common implementation issues and solutions
- Advanced prompting troubleshooting
- Testing and validation best practices
- Tool-specific guidance

---

## Facilitator Preparation Checklist

### Before the Session:
- [ ] Prepare implementation planning templates
- [ ] Set up breakout rooms or group arrangements
- [ ] Have troubleshooting guides ready
- [ ] Prepare examples of good implementations
- [ ] Test common AI tools for any updates

### During the Session:
- [ ] Monitor individual progress and provide support
- [ ] Help with advanced prompting techniques
- [ ] Facilitate effective peer feedback
- [ ] Take notes on common challenges
- [ ] Ensure everyone participates in feedback

### After the Session:
- [ ] Share session recording (if recorded)
- [ ] Post implementation resources and templates
- [ ] Follow up with participants who struggled
- [ ] Update facilitation guide based on experience

---

## Participant Materials

### Preparation Checklist:
**Before Office Hours:**
- [ ] Have capstone project idea and requirements ready
- [ ] Choose specific implementation goal for session
- [ ] Have chosen AI tools set up and ready
- [ ] Bring any relevant materials or context

**During Office Hours:**
- [ ] Set clear building goal for the session
- [ ] Use advanced prompting techniques
- [ ] Test your implementation thoroughly
- [ ] Participate actively in peer feedback

**After Office Hours:**
- [ ] Continue building based on feedback
- [ ] Apply new techniques to your project
- [ ] Share progress and challenges in cohort discussion
- [ ] Help others with their implementations

---

## Assessment Integration

### Learning Outcome Alignment:
- **Apply:** Can participants build functional components?
- **Create:** Did they develop working features?
- **Analyze:** Can they troubleshoot implementation issues?
- **Evaluate:** Can they assess implementation quality?

### Formative Assessment:
- **Observation:** Are participants building functional components?
- **Testing:** Are they validating their implementations?
- **Peer feedback:** Are they giving and receiving constructive feedback?
- **Iteration:** Are they planning improvements based on results?

### Connection to Main Session:
- Builds on advanced prompting techniques
- Provides hands-on application of quality assessment
- Prepares participants for Week 5 capstone completion

---

## Troubleshooting & Support

### Common Implementation Challenges:

**"I can't get my AI tool to work as expected"**
- Check tool setup and configuration
- Try simpler prompts first
- Use troubleshooting techniques from main session
- Ask for help from facilitator or peers

**"My output isn't what I need"**
- Try different prompting techniques
- Break down complex requests
- Use iterative refinement
- Get feedback on your approach

**"I don't know how to test my implementation"**
- Start with simple functional tests
- Use requirements as test criteria
- Try different scenarios or inputs
- Ask peers for testing suggestions

**"I'm not sure if my implementation is good enough"**
- Focus on functional requirements first
- Get peer feedback on quality
- Remember: iteration is part of the process
- Set realistic expectations for session time

### Getting Help:
- **Facilitator support:** Ask for individual assistance
- **Peer collaboration:** Work with others facing similar challenges
- **Cohort discussion:** Share challenges and get community help
- **Resource library:** Check Week 4 materials for guidance

---

## Success Metrics

### Immediate Success:
- 90% of participants build something functional
- 85% actively test their implementations
- 80% participate in peer feedback
- Active collaboration and knowledge sharing

### Learning Indicators:
- Participants can use advanced prompting techniques
- They understand how to test implementations
- They can provide constructive peer feedback
- They're planning iterations based on results

### Engagement Indicators:
- Rich discussions about implementation approaches
- Participants helping each other solve problems
- Questions show understanding of concepts
- Excitement about continuing to build

This office hours session provides hands-on building experience with peer collaboration, ensuring participants can create functional implementations and continue developing their capstone projects effectively.
