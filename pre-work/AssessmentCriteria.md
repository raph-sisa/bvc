# Assessment Criteria & Progress Tracking
## Beyond Vibe Coding Workshop for Product Professionals

## Overview

This document establishes comprehensive assessment criteria for participant progress throughout the 5-week workshop. It includes formative assessment tools (ongoing feedback), summative assessment opportunities (end-of-workshop evaluation), rubrics, peer review processes, progress tracking templates, and support identification criteria.

**Key Principle:** Assessment is for learning, not ranking. The focus is on growth, practical application, and building confidence—not grades or competition.

---

# Formative Assessment Tools (Ongoing)

Formative assessments happen throughout the workshop to provide feedback, identify gaps early, and support continuous improvement.

## 1. Weekly Reflection Prompts

### Week 1 Reflection
**Time Required:** 5-10 minutes
**Format:** Written response in discussion space

**Prompts:**
1. What was your biggest "aha moment" about how LLMs work?
2. Share one example of a prompt you improved using context engineering principles. What changed and why?
3. On a scale of 1-5, how comfortable are you with GitHub basics? What specific aspect is still challenging?
4. Looking at your work tasks, identify one that's a good candidate for automation and one that requires augmentation. Explain your reasoning.

**Purpose:** 
- Reinforce key concepts through reflection
- Identify comprehension gaps early
- Surface common questions for next session

---

### Week 2 Reflection
**Time Required:** 5-10 minutes
**Format:** Written response in discussion space

**Prompts:**
1. Describe a feature from your work. What questions would you ask AI to help architect it?
2. What tool would you choose for a prototype vs. a production project? Explain the trade-offs.
3. Share one insight from reviewing a peer's requirements document.
4. What part of architectural thinking feels most challenging? What would help?

**Purpose:**
- Apply frameworks to real work scenarios
- Practice articulating technical decisions
- Learn from peer work and feedback

---

### Week 3 Reflection
**Time Required:** 5-10 minutes
**Format:** Written response in discussion space

**Prompts:**
1. Identify an AI feature you use regularly. Does it preserve or remove user agency? How could it be improved?
2. What ethical consideration surprised you this week? How will it affect how you build with AI?
3. Share one thing you learned from collaborating with someone in a different role (PM/designer/project manager).
4. On a scale of 1-5, how confident are you in designing ethically and user-centered AI features?

**Purpose:**
- Apply critical thinking to real-world examples
- Develop ethical awareness
- Foster cross-role learning

---

### Week 4 Reflection
**Time Required:** 5-10 minutes
**Format:** Written response in discussion space

**Prompts:**
1. Share your biggest challenge this week when building with AI tools. How did you overcome it (or what help do you need)?
2. Show a before/after example of how you improved a prompt or output through iteration.
3. What optimization technique worked best for you? Share it with the group.
4. How did you test that your implementation works? What would you do differently next time?

**Purpose:**
- Document learning through struggle
- Share practical discoveries
- Build troubleshooting confidence

---

### Week 5 Reflection
**Time Required:** 10-15 minutes
**Format:** Written response in discussion space

**Prompts:**
1. How has your understanding of AI tools evolved from Week 1 to Week 5?
2. What concept or skill do you feel most confident about? What still needs practice?
3. How do you plan to apply what you've learned in your work in the next 30 days?
4. What advice would you give someone starting this workshop?
5. What's your next learning goal with AI tools?

**Purpose:**
- Synthesize learning journey
- Identify continued learning needs
- Prepare for post-workshop application

---

## 2. Self-Check Quizzes

### Week 1 Self-Check
**Time Required:** 3-5 minutes
**Format:** Quick online quiz (not graded, for self-awareness only)

**Sample Questions:**
1. LLMs are best described as: (a) Search engines (b) Prediction machines (c) Knowledge databases (d) Fact checkers
2. When a context window fills up, the AI will: (a) Expand automatically (b) Compress earlier content (c) May "forget" earlier conversation (d) Stop responding
3. Which should NOT be shared with public AI tools: (a) General coding questions (b) Proprietary code (c) Public examples (d) Learning resources
4. True/False: Tasks that are repeatable and low-risk are good candidates for automation.

**Purpose:**
- Quick knowledge check
- Identify misconceptions early
- Encourage self-awareness of learning gaps

---

### Week 3 Self-Check
**Time Required:** 3-5 minutes
**Format:** Scenario-based assessment

**Sample Scenarios:**
1. An AI feature automatically schedules meetings based on participant availability without asking for confirmation. This is an example of: (a) Good automation (b) Removed user agency (c) Ethical AI design (d) Design thinking
2. You're building a feature and realize the AI training data may have bias. Your best action is: (a) Ignore it if results seem good (b) Test with diverse scenarios (c) Assume AI companies fixed it (d) Avoid AI entirely
3. True/False: Transparency about AI involvement is always better than hiding it for smoother UX.

**Purpose:**
- Apply concepts to real scenarios
- Develop judgment and decision-making
- Identify areas needing more discussion

---

## 3. Peer Feedback Sessions

### Structure
**Frequency:** At least once in Weeks 2, 3, and 4
**Time Required:** 20-30 minutes
**Format:** Structured peer review with guiding questions

### Week 2: Requirements Document Review
**What to Review:** A peer's requirements document for a feature

**Guiding Questions:**
- Are the requirements clear and specific?
- Can you identify any missing edge cases?
- Are user stories written from the user's perspective?
- Do acceptance criteria cover the functionality?
- What's one strength and one area for improvement?

**Feedback Format:**
- 2 specific things done well
- 1-2 constructive suggestions
- 1 question for clarification

---

### Week 3: Design Review for User Agency
**What to Review:** A peer's AI feature design or user flow

**Guiding Questions:**
- Where does the user maintain control and decision-making?
- Where might user agency be compromised?
- Are ethical considerations addressed?
- Is AI involvement transparent to users?
- What's one way to improve user-centeredness?

**Feedback Format:**
- Identify where user agency is preserved (specific examples)
- Suggest one improvement for user control
- Note one ethical consideration addressed well

---

### Week 4: Implementation Review
**What to Review:** A peer's working prototype or feature

**Guiding Questions:**
- Does it work as described in the requirements?
- What testing approach did they use?
- What tool choices did they make and why?
- What's one thing you learned from their approach?
- What would you do differently and why?

**Feedback Format:**
- Note what's working well
- Share one alternative approach or idea
- Ask one question about their process

---

## 4. Exit Tickets

### Format
**Frequency:** End of each main session (60 min)
**Time Required:** 2 minutes
**Delivery:** Quick online form or chat message

**Standard Questions:**
1. One thing I learned today:
2. One thing I'm still confused about:
3. One thing I want to try this week:

**Purpose:**
- Quick pulse check on understanding
- Surface common confusion points for next session
- Encourage immediate application

---

# Summative Assessment Opportunities

Summative assessments evaluate overall learning at the end of the workshop.

## Capstone Project

### Overview
**Due:** Week 5
**Format:** Self-directed project applying workshop concepts
**Scope:** Flexible based on participant experience level

### Project Options (Choose One)

**Option 1: Prototype a Feature**
Build a working prototype of a product feature using AI tools
- Define requirements and architecture
- Select appropriate tools
- Build and test the feature
- Document your process and learnings

**Option 2: Design an AI-Enhanced Experience**
Design a complete user experience for an AI-enhanced feature
- Create user flows and wireframes
- Document ethical considerations
- Show how user agency is preserved
- Present design rationale

**Option 3: Optimize a Workflow**
Redesign an existing workflow using AI tools
- Document current vs. optimized workflow
- Show which tasks are automated vs. augmented
- Demonstrate time/quality improvements
- Create implementation guide for your team

**Option 4: Open Proposal**
Propose your own project that demonstrates workshop concepts
- Get approval in Week 3
- Must incorporate at least 3 key workshop concepts
- Must produce a tangible deliverable

---

### Capstone Project Rubric

**Category: Foundational Understanding (30%)**

**Exemplary (90-100%):**
- Clearly demonstrates understanding of LLM capabilities and limitations
- Shows sophisticated context engineering throughout
- Security considerations are thoughtfully addressed
- Augmentation vs. automation decisions are well-reasoned

**Proficient (70-89%):**
- Shows solid understanding of LLM basics
- Uses context engineering effectively most of the time
- Security considerations are mentioned and addressed
- Makes appropriate augmentation vs. automation choices

**Developing (50-69%):**
- Shows basic understanding of LLM concepts
- Attempts context engineering with some success
- Basic security awareness present
- Some confusion about when to automate vs. augment

**Beginning (0-49%):**
- Limited demonstration of foundational concepts
- Minimal evidence of context engineering
- Security considerations not addressed
- Unclear reasoning about automation vs. augmentation

---

**Category: Application of Intermediate Concepts (25%)**

**Exemplary (90-100%):**
- Demonstrates strong architectural thinking
- Requirements are comprehensive and well-structured
- Tool selection is appropriate and justified
- Prototype vs. production trade-offs are articulated

**Proficient (70-89%):**
- Shows solid architectural approach
- Requirements cover key functionality
- Tool selection is appropriate
- Understands prototype vs. production differences

**Developing (50-69%):**
- Basic architectural thinking evident
- Requirements are present but incomplete
- Tool selection is reasonable but not well justified
- Some understanding of prototype vs. production

**Beginning (0-49%):**
- Limited architectural thinking
- Requirements are vague or missing
- Tool selection unclear or inappropriate
- No clear understanding of prototype vs. production

---

**Category: Design & Ethics (20%)**

**Exemplary (90-100%):**
- User agency is thoughtfully preserved throughout
- Ethical considerations are deeply integrated
- Design thinking principles clearly applied
- Human-centered approach is evident

**Proficient (70-89%):**
- User agency is generally preserved
- Key ethical considerations are addressed
- Design thinking principles are present
- Shows user-centered thinking

**Developing (50-69%):**
- Some attention to user agency
- Basic ethical awareness shown
- Limited design thinking application
- Some user-centered considerations

**Beginning (0-49%):**
- Little attention to user agency
- Ethical considerations not addressed
- Design thinking not evident
- Limited user-centered focus

---

**Category: Practical Implementation (15%)**

**Exemplary (90-100%):**
- Produces a fully functional output (appropriate to scope)
- Clear evidence of testing and iteration
- Documentation is thorough and helpful
- Demonstrates optimization and advanced techniques

**Proficient (70-89%):**
- Produces a working output with minor issues
- Shows testing and some iteration
- Documentation covers key points
- Uses tools effectively

**Developing (50-69%):**
- Produces a partially working output
- Limited testing evidence
- Basic documentation present
- Tool usage is functional but basic

**Beginning (0-49%):**
- Output has significant functionality issues
- No clear testing approach
- Documentation is minimal or unclear
- Tool usage shows limited understanding

---

**Category: Reflection & Growth (10%)**

**Exemplary (90-100%):**
- Articulates clear learning journey with specific examples
- Identifies strengths and growth areas accurately
- Has concrete plan for continued learning
- Shows metacognitive awareness (thinking about thinking)

**Proficient (70-89%):**
- Describes learning journey clearly
- Identifies some strengths and areas for growth
- Has general plan for continued learning
- Shows self-awareness

**Developing (50-69%):**
- Mentions some learning points
- Limited awareness of strengths/weaknesses
- Vague plans for continued learning
- Basic self-reflection

**Beginning (0-49%):**
- Minimal reflection on learning
- Little self-awareness evident
- No clear plan for next steps
- Surface-level thinking

---

**Total Score Interpretation:**

- **90-100%: Exemplary** - Exceeded workshop expectations; ready for independent application
- **70-89%: Proficient** - Met workshop expectations; ready to apply with some support
- **50-69%: Developing** - Made progress but needs continued practice and support
- **0-49%: Beginning** - Needs significant additional support and practice

**Important Note:** This rubric is for self-assessment and growth feedback, NOT for ranking or exclusion. All participants who complete the workshop have succeeded in building new skills.

---

## Portfolio Development Option

For participants who want to showcase their learning publicly:

**Portfolio Components:**
1. **Project Showcase:** Capstone project with description and learnings
2. **Before/After Examples:** Show prompt improvements or workflow optimizations
3. **Reflection Essay:** Learning journey and key takeaways
4. **Resource Collection:** Curated list of tools and resources you found valuable
5. **Case Study:** Deep dive into one concept applied to real work

**Purpose:**
- Create shareable artifact for career development
- Document learning for future reference
- Contribute to broader community knowledge

---

# Progress Tracking Templates

## Individual Progress Tracker

### Weekly Progress Log

**Week:** ______

**Main Session:**
- ☐ Attended / ☐ Watched recording
- Key concepts understood: _______________
- Questions or confusion: _______________

**Recorded Video:**
- ☐ Watched
- Framework/tool learned: _______________
- How I'll apply it: _______________

**Office Hours:**
- ☐ Attended
- What I practiced: _______________
- What I got help with: _______________

**Assignment:**
- ☐ Completed required assignment
- ☐ Completed optional work
- Time spent: _______________

**Self-Assessment:**
- Confidence level (1-5): ___
- Concepts mastered this week: _______________
- Need more practice with: _______________

**Notes for Next Week:**
_______________

---

## Facilitator Progress Tracking

### Participant Observation Checklist

**Participant Name:** _______________  
**Week:** ______

**Engagement Indicators:**
- ☐ Attended main session
- ☐ Watched recorded video (indicated by discussion participation)
- ☐ Attended office hours
- ☐ Completed weekly reflection
- ☐ Participated in peer feedback
- ☐ Submitted assignment (if required)

**Understanding Indicators:**
- ☐ Asks clarifying questions appropriately
- ☐ Provides helpful peer feedback
- ☐ Shares relevant insights in discussions
- ☐ Applies concepts to own work examples
- ☐ Self-assessment aligns with demonstrated skills

**Concern Indicators (see support criteria below):**
- ☐ Missing multiple sessions
- ☐ Not participating in discussions
- ☐ Confusion on foundational concepts
- ☐ Not completing assignments
- ☐ Expresses feeling overwhelmed

**Action Items:**
_______________

---

# Peer Review Process for Collaborative Learning

## Peer Review Structure

### Pairing Strategy
- **Week 2:** Random pairing to encourage broad connections
- **Week 3:** Role-based pairing (PM with designer, etc.) for cross-disciplinary learning
- **Week 4:** Skill-based pairing (match someone strong in an area with someone developing that skill)

### Peer Review Protocol

**Step 1: Share (5 minutes)**
- Share your work with peer via GitHub, shared doc, or screen share
- Provide context: What were you trying to accomplish?

**Step 2: Review (10 minutes)**
- Peer reviews using guiding questions for that week
- Takes notes on feedback to provide

**Step 3: Feedback (10 minutes)**
- Structured feedback using format:
  - "I noticed..." (observations)
  - "I appreciate..." (strengths)
  - "I wonder..." (suggestions as questions, not directives)
  - "What if..." (alternatives to consider)

**Step 4: Reflection (5 minutes)**
- Recipient reflects: What resonated? What will you try?
- Both partners note one learning from the exchange

---

## Peer Feedback Guidelines

**Effective Peer Feedback Is:**
- ✅ Specific (references particular elements)
- ✅ Actionable (recipient can do something with it)
- ✅ Kind (supportive, not judgmental)
- ✅ Balanced (strengths and growth areas)
- ✅ Question-based (invites thinking, not prescriptive)

**Peer Feedback Is NOT:**
- ❌ Vague ("good job" or "needs work")
- ❌ Judgmental ("this is wrong")
- ❌ Comparative ("mine is better")
- ❌ Prescriptive ("you must do it this way")
- ❌ Only positive or only critical

**Example Feedback:**

❌ Poor: "This is good."

✅ Good: "I appreciate how you broke down the requirements into clear user stories. I wonder if you've considered the edge case where a user has multiple accounts? What if you added that to the acceptance criteria?"

---

# Identifying Participants Who Need Additional Support

## Early Warning Indicators

### Week 1 Red Flags
- Did not complete GitHub setup despite office hours support
- Unable to explain basic LLM concepts in reflection
- No participation in discussion space
- Expressed feeling completely overwhelmed
- Missing multiple components (main session + office hours + assignment)

### Week 2 Red Flags
- Still struggling with foundational Week 1 concepts
- Unable to complete requirement gathering exercise even with template
- No peer feedback provided
- Expressed that content is "too technical"
- Disengaged in office hours

### Week 3-4 Red Flags
- Not making progress on any practical implementation
- Unable to test or iterate on AI outputs
- Minimal peer interaction
- Expressed desire to drop out
- Significant concept confusion persists

---

## Support Intervention Strategies

### Level 1: Gentle Check-In
**When:** Missing 1-2 components in a week OR expressed mild confusion

**Action:**
- Send personal message: "Noticed you missed [X]. Everything okay?"
- Offer office hours time
- Share relevant recorded content
- Connect with peer buddy

---

### Level 2: Structured Support
**When:** Missing 3+ components in a week OR foundational concept confusion persists

**Action:**
- Schedule 1-on-1 check-in (15-30 min)
- Assess specific blockers
- Create personalized catch-up plan
- Simplify assignment scope if needed
- Pair with mentor participant
- Consider if they need beginner pre-work

---

### Level 3: Intensive Support or Alternative Path
**When:** Falling significantly behind OR expressed strong overwhelm

**Action:**
- Frank conversation about goals and constraints
- Options discussion:
  - Intensive catch-up with dedicated support
  - Adjust expectations to "audit mode" (learn what you can, no pressure)
  - Take a pause and rejoin future cohort
- Focus on 1-2 key concepts rather than all content
- Celebrate any progress made

---

## Support Success Indicators

**Participant is getting back on track when:**
- ☐ Re-engages with discussion space
- ☐ Completes at least one component per week
- ☐ Expresses increased confidence or clarity
- ☐ Asks specific questions (not general overwhelm)
- ☐ Shows up to office hours
- ☐ Demonstrates understanding of at least foundational concepts

---

# Assessment Philosophy Summary

## Core Principles

1. **Growth Over Performance:** We measure progress, not perfection
2. **Multiple Pathways:** Different participants will excel in different areas
3. **Practical Application:** Assessment focuses on real-world applicability
4. **Self-Direction:** Participants drive their own assessment and goal-setting
5. **Psychological Safety:** Assessment is for learning, not judgment
6. **Flexibility:** Accommodate different experience levels and learning paces

## Assessment Serves Learning When:

- ✅ Participants can self-assess accurately
- ✅ Feedback is timely and actionable
- ✅ Struggles are normalized and supported
- ✅ Success is defined broadly and inclusively
- ✅ Assessment informs teaching (not just evaluation)
- ✅ Participants feel safe to take risks and experiment

---

# Quick Reference: Assessment Calendar

| Week | Formative Assessment | Summative Assessment | Peer Review |
|------|---------------------|----------------------|-------------|
| 1 | Reflection prompts, Self-check quiz, Exit tickets | - | - |
| 2 | Reflection prompts, Exit tickets | - | Requirements doc review |
| 3 | Reflection prompts, Self-check quiz, Exit tickets | - | Design review |
| 4 | Reflection prompts, Exit tickets | - | Implementation review |
| 5 | Final reflection, Capstone presentation | Capstone project, Portfolio (optional) | Peer feedback on capstones |

---

# Conclusion

This comprehensive assessment framework ensures:
- ✅ Regular feedback for continuous improvement (formative)
- ✅ Clear evaluation of overall learning (summative)
- ✅ Multiple opportunities for self-assessment
- ✅ Structured peer learning and feedback
- ✅ Early identification of participants needing support
- ✅ Flexible, growth-oriented approach to evaluation

Assessment is designed to support learning, build confidence, and provide actionable feedback—never to rank, judge, or exclude participants. Every participant who engages with the workshop content is succeeding in building valuable new skills.

